{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "cuda_is_available = torch.cuda.is_available()\n",
        "\n",
        "if cuda_is_available:\n",
        "  print(\"All good!\")\n",
        "else:\n",
        "  print(\"CUDA is NOT available!\")"
      ],
      "metadata": {
        "id": "ulgnkbbvEt6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"During the latest presentation OpenAI\"\n",
        "model = \"openai-community/gpt2-large\""
      ],
      "metadata": {
        "id": "7R6RnqQu0-P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the text generation pipeline\n",
        "text_generator = pipeline(\"text-generation\", model=model, device=0)"
      ],
      "metadata": {
        "id": "hkVKzDegAItu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "print(f\"Generated text:\\n{generated_texts[0]['generated_text']}\")"
      ],
      "metadata": {
        "id": "_xUAq3XiAN1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for do_sample in [\n",
        "    False, # Greedy Search\n",
        "    True   # Multinomial sampling\n",
        "  ]:\n",
        "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=do_sample, num_beams=1)\n",
        "\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Parameters:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(f\"do_sample={do_sample}\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Generation:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(generated_texts[0]['generated_text'])\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "rGcDDPcz1Ln5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Beam-search strategy\n",
        "\n",
        "for beams in [1, 2, 4, 8]:\n",
        "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=False, num_beams=beams)\n",
        "\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Parameters:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(f\"num_beams={beams}\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Generation:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(generated_texts[0]['generated_text'])\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "uo3CC3yk4QmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Beam-search multinomial sampling\n",
        "\n",
        "for beams in [1, 2, 4, 8]:\n",
        "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, num_beams=beams)\n",
        "\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Parameters:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(f\"num_beams={beams}\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Generation:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(generated_texts[0]['generated_text'])\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "K96hVRPC6MUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the top-k parameter\n",
        "\n",
        "for k in [1, 5, 10, 50, 100, 500]:\n",
        "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, top_k=k)\n",
        "\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Parameters:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(f\"top_k={k}\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Generation:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(generated_texts[0]['generated_text'])\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "f5R7GeD3AUgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change temperature\n",
        "\n",
        "for temp in [0.1, 1.0, 2.0, 3.0]:\n",
        "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, temperature=temp)\n",
        "\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Parameters:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(f\"temperature={temp}\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"Generation:\")\n",
        "  print(\"-----------------------------------\")\n",
        "  print(generated_texts[0]['generated_text'])\n",
        "  print(\"-----------------------------------\")\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "yXOuaxe0RJwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UUiu5ZdT9ArW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}